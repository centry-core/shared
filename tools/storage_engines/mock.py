# pylint: disable=C0116
#
#   Copyright 2023 getcarrier.io
#
#   Licensed under the Apache License, Version 2.0 (the "License");
#   you may not use this file except in compliance with the License.
#   You may obtain a copy of the License at
#
#       http://www.apache.org/licenses/LICENSE-2.0
#
#   Unless required by applicable law or agreed to in writing, software
#   distributed under the License is distributed on an "AS IS" BASIS,
#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#   See the License for the specific language governing permissions and
#   limitations under the License.

""" Secret engine impl """

import os
import datetime

from pylon.core.tools import log  # pylint: disable=E0401

from tools import context  # pylint: disable=E0401
from tools import config as c  # pylint: disable=E0401


class MockMeta(type):
    """ Client meta class """

    def __getattr__(cls, name):
        log.info("cls.__getattr__(%s)", name)


class EngineBase(metaclass=MockMeta):
    """ Client mock / debug base class """

    def __getattr__(self, name):
        log.info("base.__getattr__(%s)", name)

    def __init__(self, *args, **kwargs):
        _ = args, kwargs
        #
        self.bucket_path = os.path.join(c.MOCK_STORAGE_PATH, "bucket")
        self.meta_path = os.path.join(c.MOCK_STORAGE_PATH, "meta")
        #
        os.makedirs(self.bucket_path, exist_ok=True)
        os.makedirs(self.meta_path, exist_ok=True)

    def extract_access_data(self, integration_id=None, is_local=True):
        raise RuntimeError("Not supported")

    @property
    def bucket_prefix(self):
        raise NotImplementedError

    def format_bucket_name(self, bucket):
        if bucket.startswith(self.bucket_prefix):
            return bucket
        #
        return f"{self.bucket_prefix}{bucket}"

    def list_bucket(self):
        return [
            item.replace(self.bucket_prefix, "", 1)
            for item in os.listdir(path=self.bucket_path)
            if item.startswith(self.bucket_prefix)
        ]

    def create_bucket(self, bucket, bucket_type=None, retention_days=None):
        bucket_name = self.format_bucket_name(bucket)
        path = os.path.join(self.bucket_path, bucket_name)
        #
        os.makedirs(path, exist_ok=True)
        #
        # if bucket_type and bucket_type in ("system", "autogenerated", "local"):
        #     self.set_bucket_tags(bucket=bucket, tags={"type": bucket_type})
        #
        # if retention_days:
        #     self.configure_bucket_lifecycle(bucket_name, retention_days)
        #
        # No return dict data emulated

    def list_files(self, bucket, next_continuation_token=None):
        _ = next_continuation_token
        #
        bucket_name = self.format_bucket_name(bucket)
        path = os.path.join(self.bucket_path, bucket_name)
        #
        files = []
        #
        with os.scandir(path) as it:
            for entry in it:
                stat = entry.stat()
                #
                files.append({
                    "name": entry.name,
                    "size": stat.st_size,
                    "modified": datetime.datetime.fromtimestamp(stat.st_mtime).isoformat(),
                })
        #
        return files

    # @space_monitor
    def upload_file(self, bucket, file_obj, file_name):
        bucket_name = self.format_bucket_name(bucket)
        path = os.path.join(self.bucket_path, bucket_name, file_name)
        #
        with open(path, "wb") as file:
            if isinstance(file_obj, bytes):
                data = file_obj
            elif isinstance(file_obj, str):
                data = file_obj.encode()
            else:
                data = file_obj.read()
            #
            file.write(data)
        #
        # throughput_monitor(client=self, file_size=sys.getsizeof(file_obj))
        #
        # No return response data emulated

    # def download_file(self, bucket: str, file_name: str, project_id: int = None) -> bytes:
    #     response = self.s3_client.get_object(Bucket=self.format_bucket_name(bucket), Key=file_name)
    #     throughput_monitor(client=self, file_size=response['ContentLength'], project_id=project_id)
    #     return response["Body"].read()

    # @space_monitor
    # def remove_file(self, bucket: str, file_name: str):
    #     # self._space_monitor()
    #     return self.s3_client.delete_object(Bucket=self.format_bucket_name(bucket), Key=file_name)

    # def remove_bucket(self, bucket: str):
    #     # self._space_monitor()
    #     for file_obj in self.list_files(bucket):
    #         self.remove_file(bucket, file_obj["name"])
    #     self.s3_client.delete_bucket(Bucket=self.format_bucket_name(bucket))

    # def configure_bucket_lifecycle(self, bucket: str, days: int) -> None:
    #     self.s3_client.put_bucket_lifecycle_configuration(
    #         Bucket=self.format_bucket_name(bucket),
    #         LifecycleConfiguration={
    #             "Rules": [
    #                 {
    #                     "Expiration": {
    #                         # "NoncurrentVersionExpiration": days,
    #                         "Days": days
    #                         # "ExpiredObjectDeleteMarker": True
    #                     },
    #                     "NoncurrentVersionExpiration": {
    #                         'NoncurrentDays': days
    #                     },
    #                     "ID": "bucket-retention-policy",
    #                     'Filter': {'Prefix': ''},
    #                     "Status": "Enabled"
    #                 }
    #             ]
    #         }
    #     )

    # def get_bucket_lifecycle(self, bucket: str) -> dict:
    #     return self.s3_client.get_bucket_lifecycle(Bucket=self.format_bucket_name(bucket))

    def get_bucket_size(self, bucket):
        bucket_name = self.format_bucket_name(bucket)
        path = os.path.join(self.bucket_path, bucket_name)
        #
        total_size = 0
        #
        with os.scandir(path) as it:
            for entry in it:
                stat = entry.stat()
                #
                total_size += stat.st_size
        #
        return total_size

    def get_file_size(self, bucket, filename):
        bucket_name = self.format_bucket_name(bucket)
        path = os.path.join(self.bucket_path, bucket_name, filename)
        #
        stat = os.stat(path)
        return stat.st_size

    def get_bucket_tags(self, bucket):
        log.info("get_bucket_tags(%s)", bucket)
        return {}

    # def set_bucket_tags(self, bucket: str, tags: dict) -> None:
    #     tag_set = [{'Key': k, 'Value': v} for k, v in tags.items()]
    #     self.s3_client.put_bucket_tagging(
    #         Bucket=self.format_bucket_name(bucket),
    #         Tagging={
    #             'TagSet': tag_set
    #         },
    #     )

    def select_object_content(self, bucket, file_name, expression_addon=""):
        log.info("select_object_content(%s, %s, %s)", bucket, file_name, expression_addon)
        return []

    def is_file_exist(self, bucket, file_name):
        bucket_name = self.format_bucket_name(bucket)
        path = os.path.join(self.bucket_path, bucket_name, file_name)
        #
        return os.path.exists(path)


class Engine(EngineBase):
    """ Client mock / debug class """

    def __init__(self, project, integration_id=None, is_local=True, **kwargs):
        _ = kwargs
        #
        self.project = project
        self.integration_id = integration_id
        self.is_local = is_local
        #
        super().__init__()

    @property
    def bucket_prefix(self):
        return f"p--{self.project.id}."

    @classmethod
    def from_project_id(
            cls, project_id,
            integration_id=None, is_local=True, rpc_manager=None,
            **kwargs
    ):
        log.info("from_project_id(%s, %s, %s, %s, %s)", project_id, integration_id, is_local, rpc_manager, kwargs)  # pylint: disable=C0301
        #
        if not rpc_manager:
            rpc_manager = context.rpc_manager
        #
        project = rpc_manager.call.project_get_or_404(project_id=project_id)
        return cls(project, integration_id, is_local)


class AdminEngine(EngineBase):
    """ Client mock / debug class """

    def __init__(self, integration_id=None, **kwargs):
        _ = kwargs
        #
        self.project = None
        self.integration_id = integration_id
        self.is_local = False
        #
        super().__init__()

    @property
    def bucket_prefix(self):
        return "p--administration."
