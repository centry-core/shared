from abc import abstractmethod, ABC
from json import loads
from queue import Empty
from typing import Optional
import sys

import boto3
from botocore.client import Config, ClientError
from pylon.core.tools import log

from .rpc_tools import RpcMixin, EventManagerMixin

from tools import config as c


def space_monitor(f):
    def wrapper(*args, **kwargs):
        client = args[0]
        bucket = kwargs['bucket'] if kwargs.get('bucket') else args[1]
        size_before = client.get_bucket_size(bucket)
        result = f(*args, **kwargs)
        size_after = client.get_bucket_size(bucket)
        payload = {
            'project_id': client.project.id if client.project else None,
            'current_delta': size_after - size_before, 
            'integration_id': client.integration_id,
            'is_local': client.is_local
        }
        client.event_manager.fire_event('usage_space_monitor', payload)
        return result
    return wrapper
        


class MinioClientABC(ABC):
    PROJECT_SECRET_KEY: str = "minio_aws_access"
    TASKS_BUCKET: str = "tasks"
    project = None

    def __init__(self,
                 aws_access_key_id: str = c.MINIO_ACCESS,
                 aws_secret_access_key: str = c.MINIO_SECRET,
                 region_name: str = c.MINIO_REGION,
                 endpoint_url: str = c.MINIO_URL,
                 **kwargs
                 ):
        self.s3_client = boto3.client(
            "s3", endpoint_url=endpoint_url,
            aws_access_key_id=aws_access_key_id,
            aws_secret_access_key=aws_secret_access_key,
            config=Config(signature_version="s3v4"),
            region_name=region_name
        )
        self.event_manager = EventManagerMixin().event_manager

    def extract_access_data(self, integration_id: Optional[int] = None, is_local: bool = True) -> tuple:
        rpc_manager = RpcMixin().rpc
        try:
            if self.project:
                settings = rpc_manager.timeout(2).integrations_get_s3_settings(
                    self.project.id, integration_id, is_local)
            else:
                settings = rpc_manager.timeout(2).integrations_get_s3_admin_settings(
                    integration_id)
        except Empty:
            settings = None
        if settings:
            self.integration_id = settings['integration_id']
            self.is_local = settings['is_local']
            return (
                settings['access_key'],
                settings['secret_access_key'],
                settings['region_name'],
                settings['storage_url'] if settings['use_compatible_storage'] else None
            )
        return c.MINIO_ACCESS, c.MINIO_SECRET, c.MINIO_REGION, c.MINIO_URL

    @property
    @abstractmethod
    def bucket_prefix(self) -> str:
        raise NotImplementedError

    def format_bucket_name(self, bucket: str) -> str:
        if bucket.startswith(self.bucket_prefix):
            return bucket
        return f"{self.bucket_prefix}{bucket}"

    def list_bucket(self) -> list:
        return [
            each["Name"].replace(self.bucket_prefix, "", 1)
            for each in self.s3_client.list_buckets().get("Buckets", {})
            if each["Name"].startswith(self.bucket_prefix)
        ]

    def create_bucket(self, bucket: str, bucket_type=None, retention_days: Optional[int] = None) -> Optional[dict]:
        try:
            bucket_name = self.format_bucket_name(bucket)
            response = self.s3_client.create_bucket(
                ACL="public-read",
                Bucket=bucket_name,
                CreateBucketConfiguration={"LocationConstraint": c.MINIO_REGION}
            )
            if bucket_type and bucket_type in ('system', 'autogenerated', 'local'):
                self.set_bucket_tags(bucket=bucket, tags={'type': bucket_type})
            if retention_days:
                self.configure_bucket_lifecycle(bucket_name, retention_days)
            return response
        except ClientError as client_error:
            log.warning(client_error)
        except Exception as exc:
            log.error(exc)

    def list_files(self, bucket: str, next_continuation_token: Optional[str] = None) -> list:
        if next_continuation_token:
            response = self.s3_client.list_objects_v2(
                Bucket=self.format_bucket_name(bucket),
                ContinuationToken=next_continuation_token
            )
        else:
            response = self.s3_client.list_objects_v2(
                Bucket=self.format_bucket_name(bucket)
            )
        files = [
            {
                "name": each["Key"],
                "size": each["Size"],
                "modified": each["LastModified"].isoformat()
            }
            for each in response.get("Contents", [])
        ]
        continuation_token = response.get("NextContinuationToken")
        if continuation_token and response["Contents"]:
            files.extend(self.list_files(bucket, next_continuation_token=continuation_token))
        return files

    @space_monitor
    def upload_file(self, bucket: str, file_obj: bytes, file_name: str):
        response = self.s3_client.put_object(Key=file_name, Bucket=self.format_bucket_name(bucket), Body=file_obj)
        self._throughput_monitor(file_size=sys.getsizeof(file_obj))
        # self._space_monitor()
        return response

    def download_file(self, bucket: str, file_name: str, project_id: int = None) -> bytes:
        response = self.s3_client.get_object(Bucket=self.format_bucket_name(bucket), Key=file_name)
        self._throughput_monitor(file_size=response['ContentLength'], project_id=project_id)
        return response["Body"].read()

    @space_monitor
    def remove_file(self, bucket: str, file_name: str):
        # self._space_monitor()
        return self.s3_client.delete_object(Bucket=self.format_bucket_name(bucket), Key=file_name)

    def remove_bucket(self, bucket: str):
        # self._space_monitor()
        for file_obj in self.list_files(bucket):
            self.remove_file(bucket, file_obj["name"])

        self.s3_client.delete_bucket(Bucket=self.format_bucket_name(bucket))

    def configure_bucket_lifecycle(self, bucket: str, days: int) -> None:
        self.s3_client.put_bucket_lifecycle_configuration(
            Bucket=self.format_bucket_name(bucket),
            LifecycleConfiguration={
                "Rules": [
                    {
                        "Expiration": {
                            # "NoncurrentVersionExpiration": days,
                            "Days": days
                            # "ExpiredObjectDeleteMarker": True
                        },
                        "NoncurrentVersionExpiration": {
                            'NoncurrentDays': days
                        },
                        "ID": "bucket-retention-policy",
                        'Filter': {'Prefix': ''},
                        "Status": "Enabled"
                    }
                ]
            }
        )

    def get_bucket_lifecycle(self, bucket: str) -> dict:
        return self.s3_client.get_bucket_lifecycle(Bucket=self.format_bucket_name(bucket))

    def get_bucket_size(self, bucket: str) -> int:
        total_size = 0
        for each in self.s3_client.list_objects_v2(
                Bucket=self.format_bucket_name(bucket)
        ).get('Contents', {}):
            total_size += each["Size"]
        return total_size

    def get_file_size(self, bucket: str, filename: str) -> int:
        response = self.s3_client.list_objects_v2(Bucket=self.format_bucket_name(bucket)).get("Contents", {})
        for i in response:
            if str(i["Key"]).lower() == str(filename).lower():
                return i["Size"]
        return 0

    def get_bucket_tags(self, bucket: str) -> dict:
        try:
            return self.s3_client.get_bucket_tagging(Bucket=self.format_bucket_name(bucket))
        except ClientError:
            return {}

    def set_bucket_tags(self, bucket: str, tags: dict) -> None:
        tag_set = [{'Key': k, 'Value': v} for k, v in tags.items()]
        self.s3_client.put_bucket_tagging(
            Bucket=self.format_bucket_name(bucket),
            Tagging={
                'TagSet': tag_set
            },
        )

    def select_object_content(self, bucket: str, file_name: str, expression_addon: str = '') -> list:
        try:
            response = self.s3_client.select_object_content(
                Bucket=self.format_bucket_name(bucket),
                Key=file_name,
                ExpressionType='SQL',
                Expression=f"select * from s3object s{expression_addon}",
                InputSerialization={
                    'CSV': {
                        "FileHeaderInfo": "USE",
                    },
                    'CompressionType': 'GZIP',
                },
                OutputSerialization={'JSON': {}},
            )
        except ClientError as ex:
            if ex.response['Error']['Code'] == 'NoSuchKey':
                log.error(f'Cannot find file "{file_name}" in bucket "{bucket}"')
                return []
            else:
                raise
        results = []
        for event in response['Payload']:
            if 'Records' in event:
                payload = event['Records']['Payload'].decode('utf-8')
                for line in payload.split('\n'):
                    try:
                        results.append(loads(line))
                    except Exception:
                        pass
            if 'Stats' in event:
                self._throughput_monitor(file_size=event['Stats']['Details']['BytesScanned'])
        return results

    def is_file_exist(self, bucket: str, file_name: str):
        response = self.s3_client.list_objects_v2(
            Bucket=bucket,
            Prefix=file_name,
        )
        for obj in response.get('Contents', []):
            if obj['Key'] == file_name:
                return True
        return False
    
    def _throughput_monitor(self, file_size: int, project_id: int = None):
        payload = {
            'project_id': self.project.id if self.project else project_id,
            'file_size': file_size, 
            'integration_id': self.integration_id,
            'is_local': self.is_local
        }
        self.event_manager.fire_event('usage_throughput_monitor', payload)

    # def _space_monitor(self):
    #     used_space = 0
    #     buckets = self.list_bucket()
    #     for bucket in buckets:
    #         bucket_size = self.get_bucket_size(bucket)
    #         used_space += bucket_size
    #     payload = {
    #         'project_id': self.project.id if self.project else None,
    #         'used_space': used_space, 
    #         'integration_id': self.integration_id,
    #         'is_local': self.is_local
    #     }
    #     self.event_manager.fire_event('usage_space_monitor', payload)


class MinioClientAdmin(MinioClientABC):
    def __init__(self,
                 integration_id: Optional[int] = None,
                 **kwargs):
        self.project = None
        # self.integration_id = integration_id
        # self.is_local = False
        access_key, secret_access_key, region_name, url = self.extract_access_data(integration_id)
        super().__init__(access_key, secret_access_key, region_name, url)

    @property
    def bucket_prefix(self) -> str:
        return 'p--administration.'


class MinioClient(MinioClientABC):
    @classmethod
    def from_project_id(cls, project_id: int,
                        integration_id: Optional[int] = None,
                        is_local: bool = True,
                        rpc_manager=None,
                        **kwargs):
        if not rpc_manager:
            rpc_manager = RpcMixin().rpc
        project = rpc_manager.call.project_get_or_404(project_id=project_id)
        return cls(project, integration_id, is_local)

    def __init__(self, project,
                 integration_id: Optional[int] = None,
                 is_local: bool = True,
                 **kwargs):
        self.project = project
        # self.integration_id = integration_id
        # self.is_local = is_local
        access_key, secret_access_key, region_name, url = self.extract_access_data(integration_id,
                                                                                   is_local)
        super().__init__(access_key, secret_access_key, region_name, url)

    @property
    def bucket_prefix(self) -> str:
        return f'p--{self.project.id}.'
